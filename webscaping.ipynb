{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3:  Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muhammad Tahir @ Data Science Immersive at General Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Par-1:  Imports and Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "import regex as re\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/conspiracy/ & https://www.reddit.com/r/TheOnion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_onion = 'https://www.reddit.com/r/TheOnion/.json'\n",
    "url_conspiracy = 'https://www.reddit.com/r/conspiracy/.json'\n",
    "user_header = {'User-agent': 'JaneJacobs'} # header to prevent error 429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(url, n_pulls, header):\n",
    "    # initialize empty requirements\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    # Create a loop that does a max of 25 requests per pull\n",
    "    for pull_num in range(n_pulls):\n",
    "        print(f'Pulling data attempted {pull_num+1} times')\n",
    "        \n",
    "        if (after == None):\n",
    "            new_url = url         # base case\n",
    "        else:\n",
    "            new_url = url+\"?after=\"+after   # subsequent iterations\n",
    "            \n",
    "        res = requests.get(new_url, headers = header)\n",
    "        \n",
    "        if (res.status_code == 200):\n",
    "            subreddit_json = res.json()                       # Pull json\n",
    "            posts.extend(subreddit_json['data']['children'])  # Get subreddit posts\n",
    "            after = subreddit_json['data']['after']           # after = ID of last post in iteration\n",
    "        else:\n",
    "            print(f'It would appear there has been an error. Status Code: {res.status_code}')\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n",
      "Pulling data attempted 11 times\n",
      "Pulling data attempted 12 times\n",
      "Pulling data attempted 13 times\n",
      "Pulling data attempted 14 times\n",
      "Pulling data attempted 15 times\n",
      "Pulling data attempted 16 times\n",
      "Pulling data attempted 17 times\n",
      "Pulling data attempted 18 times\n",
      "Pulling data attempted 19 times\n",
      "Pulling data attempted 20 times\n",
      "Pulling data attempted 21 times\n",
      "Pulling data attempted 22 times\n",
      "Pulling data attempted 23 times\n",
      "Pulling data attempted 24 times\n",
      "Pulling data attempted 25 times\n",
      "Pulling data attempted 26 times\n",
      "Pulling data attempted 27 times\n",
      "Pulling data attempted 28 times\n",
      "Pulling data attempted 29 times\n",
      "Pulling data attempted 30 times\n",
      "Pulling data attempted 31 times\n",
      "Pulling data attempted 32 times\n",
      "Pulling data attempted 33 times\n",
      "Pulling data attempted 34 times\n",
      "Pulling data attempted 35 times\n",
      "Pulling data attempted 36 times\n",
      "Pulling data attempted 37 times\n",
      "Pulling data attempted 38 times\n",
      "Pulling data attempted 39 times\n",
      "Pulling data attempted 40 times\n"
     ]
    }
   ],
   "source": [
    "onion_posts = get_subreddit(url_onion, n_pulls = 40, header=user_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onion_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts(posts):\n",
    "    \n",
    "    post_titles = []\n",
    "    for i in range(len(posts)):\n",
    "        titles = {}\n",
    "        titles['title']     = posts[i]['data']['title']\n",
    "        titles['subreddit'] = posts[i]['data']['subreddit']\n",
    "        titles['id']        = posts[i]['data']['id']\n",
    "        post_titles.append(titles)\n",
    "\n",
    "    return(pd.DataFrame(post_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parliamentarian Cuts Minimum Wage From Stimulu...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>lt7cvj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democrat Reassures Friend This One Of The Good...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>ltfhgo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goals Of Biden Administration Reviewing U.S. S...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>lszpl2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden Comforts Families Of Syrian Airstrike Vi...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>lt6h9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Florida GOP Introduces Ballotless Voting In Di...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>lsekit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title subreddit      id\n",
       "0  Parliamentarian Cuts Minimum Wage From Stimulu...  TheOnion  lt7cvj\n",
       "1  Democrat Reassures Friend This One Of The Good...  TheOnion  ltfhgo\n",
       "2  Goals Of Biden Administration Reviewing U.S. S...  TheOnion  lszpl2\n",
       "3  Biden Comforts Families Of Syrian Airstrike Vi...  TheOnion  lt6h9d\n",
       "4  Florida GOP Introduces Ballotless Voting In Di...  TheOnion  lsekit"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion = pull_posts(onion_posts)\n",
    "df_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n",
      "Pulling data attempted 11 times\n",
      "Pulling data attempted 12 times\n",
      "Pulling data attempted 13 times\n",
      "Pulling data attempted 14 times\n",
      "Pulling data attempted 15 times\n",
      "Pulling data attempted 16 times\n",
      "Pulling data attempted 17 times\n",
      "Pulling data attempted 18 times\n",
      "Pulling data attempted 19 times\n",
      "Pulling data attempted 20 times\n",
      "Pulling data attempted 21 times\n",
      "Pulling data attempted 22 times\n",
      "Pulling data attempted 23 times\n",
      "Pulling data attempted 24 times\n",
      "Pulling data attempted 25 times\n",
      "Pulling data attempted 26 times\n",
      "Pulling data attempted 27 times\n",
      "Pulling data attempted 28 times\n",
      "Pulling data attempted 29 times\n",
      "Pulling data attempted 30 times\n",
      "Pulling data attempted 31 times\n",
      "Pulling data attempted 32 times\n",
      "Pulling data attempted 33 times\n",
      "Pulling data attempted 34 times\n",
      "Pulling data attempted 35 times\n",
      "Pulling data attempted 36 times\n",
      "Pulling data attempted 37 times\n",
      "Pulling data attempted 38 times\n",
      "Pulling data attempted 39 times\n",
      "Pulling data attempted 40 times\n",
      "Pulling data attempted 41 times\n"
     ]
    }
   ],
   "source": [
    "conspiracy_posts = get_subreddit(url_conspiracy, n_pulls = 41, header=user_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never forget Gary Webb; The reporter who sacri...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>lap3gy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Police are required to wear body cameras how d...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>lucwwi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stating The Obvious</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>luiepa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Name one time in history the people burning bo...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ltu54k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germans scientists say that Vitamin D increase...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>lu88lj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   subreddit      id\n",
       "0  Never forget Gary Webb; The reporter who sacri...  conspiracy  lap3gy\n",
       "1  Police are required to wear body cameras how d...  conspiracy  lucwwi\n",
       "2                                Stating The Obvious  conspiracy  luiepa\n",
       "3  Name one time in history the people burning bo...  conspiracy  ltu54k\n",
       "4  Germans scientists say that Vitamin D increase...  conspiracy  lu88lj"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conspiracy = pull_posts(conspiracy_posts)\n",
    "df_conspiracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onion.drop_duplicates(subset = 'id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conspiracy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conspiracy.drop_duplicates(subset = 'id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conspiracy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2:  Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Lemmatizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(text_col):\n",
    "    # Instantiate tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'[A-z]+')\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = [tokenizer.tokenize(post.lower()) for post in text_col]\n",
    "    \n",
    "    # Instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = []\n",
    "    # lemmatize words\n",
    "    for words in tokens:\n",
    "        lem_list = [lemmatizer.lemmatize(i) for i in words]\n",
    "        lem_post = ''\n",
    "        for word in lem_list:\n",
    "            lem_post += (word+' ')\n",
    "        tokens_lem.append(lem_post.strip())\n",
    "    \n",
    "    return (tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_token_list = tokenize_lemmatize(df_onion['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goal of biden administration reviewing u s supply chain'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_token_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "lemma_features = cvec.fit_transform(lemma_token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<928x3897 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11458 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
